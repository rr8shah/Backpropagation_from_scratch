{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('D:/Canada/University of Waterloo/3. Spring 2020/ECE 657/Assignments/Assignment 1/train_data.csv', header = None)\n",
    "label = pd.read_csv('D:/Canada/University of Waterloo/3. Spring 2020/ECE 657/Assignments/Assignment 1/train_labels.csv', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.array(train)\n",
    "label = np.array(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combined the train data and label to split into training and validation data\n",
    "combined = np.c_[train,label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Train and Validation Split\n",
    "##### Features and labels are combined to split the dataset into training set and validation set. I have splitted the data set into two parts 90% and 10% for training and validation respectively. I prefered 90% and 10% considering large amount of data points i.e 24752 which will result in 22278 data points for training and 2476 datapoints for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data and labels for training dataset\n",
    "X_train = combined[:int(combined.shape[0]*0.9),:784]\n",
    "y_train = combined[:int(combined.shape[0]*0.9),784:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data and labels for validation dataset\n",
    "X_val = combined[int(combined.shape[0]*0.9):,:784]\n",
    "y_val = combined[int(combined.shape[0]*0.9):,784:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22278, 784)\n",
      "(22278, 4)\n",
      "(2476, 784)\n",
      "(2476, 4)\n"
     ]
    }
   ],
   "source": [
    "#Checking the shape of the training and Validation data\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Details about the Neural Network\n",
    "- Architecture\n",
    "##### My neural network consists of 784 input perceptron, 44 perceptron in hidden layer and 4 perceptrons in the output layer. The reason for selecting small number of neuron is with the intuation of having image dataset observing values between 0-255 in middle features. The patterns required to classify the images with lots of 0 are less resuting in small number of neurons. The exact number of neuron was obtained fine tunning the model based on the validation accuracy.</br>\n",
    "\n",
    "\n",
    " - Activation Function\n",
    "##### The activation function used in the hidden layer is the sigmoid and for output layer softmax is used considering multi class one hot encoded labels.</br>\n",
    "\n",
    "\n",
    " - Loss Function\n",
    "##### The loss function used for this multi label class problem is cross entropy as it penalizes for predicting the wrong class and always tries to minimize the distance between the actual and predicted labels.</br>\n",
    "\n",
    "\n",
    " - Back Propagation\n",
    "##### Mini Batch back propagation is implemented instead of batch backpropagation because of the faster convergence to the global monima compared to the batch backpropagation. The back propagation is carried out (Total No. of samples/Batch size) i.e 2785 time in one epoch for mini batch back propagation.</br>\n",
    "\n",
    "\n",
    " - Accuracy\n",
    "##### The Training accuracy achived is 99.94% and validation accuracy is 99.03%.</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.Nl = 2\n",
    "        self.dim = [784, 44, 4]\n",
    "        self.loss = []\n",
    "        self.param = {}\n",
    "        self.inter = {}\n",
    "        self.sample = self.y.shape[0]\n",
    "        self.lr = 0.75\n",
    "\n",
    "# Initializing the weights and bias of the requied dimensions randomly\n",
    "    def weightinit(self):\n",
    "        np.random.seed(42)\n",
    "        self.param['W1'] = np.random.randn(self.dim[0], self.dim[1])\n",
    "        self.param['b1'] = np.zeros((1, self.dim[1]))\n",
    "        self.param['W2'] = np.random.randn(self.dim[1], self.dim[2])\n",
    "        self.param['b2'] = np.zeros((1, self.dim[2]))\n",
    "        return\n",
    "\n",
    "#Sigmoid is the activation function used in the hidden layer\n",
    "    def sigmoid(self, z):\n",
    "        sig = 1 / (1 + np.exp(-z))\n",
    "        return sig\n",
    "\n",
    "#Softmax is the function used in the output layer to get the probability for each of class\n",
    "#Softmax is used because of the multi class classification problem\n",
    "    def softmax(self, z):\n",
    "        exps = np.exp(z)\n",
    "        return exps / np.sum(exps, axis=1, keepdims=True)\n",
    "\n",
    "#One sided cross entropy function is used for the multi class classification problem\n",
    "#As we have 4 classes : 0, 1,2 and 3\n",
    "    def lossfunction(self, predict, actual):\n",
    "        n_samples = actual.shape[0]\n",
    "        true_class = actual.argmax(axis=1)\n",
    "        logrithm = - np.log(predict[np.arange(n_samples), true_class])\n",
    "        cross = np.sum(logrithm) / n_samples\n",
    "        return cross\n",
    "\n",
    "#Forward function is used for the matrix calculations of forward propagation and for prediction of the class\n",
    "#Input is the N_sample X features matrix\n",
    "#Output is Predicted one hot encoded numpy array and loss\n",
    "    def Forward(self):\n",
    "        #Matrix multiplication of W1 and input features \n",
    "        z1 = np.dot(self.X, self.param['W1']) + self.param['b1']\n",
    "        #Passing it through sigmoid generates the Output for each of the perceptron in hidden layer\n",
    "        a1 = self.sigmoid(z1)\n",
    "        #Matrix multiplication of a1 and W2\n",
    "        z2 = np.dot(a1, self.param['W2']) + self.param['b2']\n",
    "        #Passing it through softmax generates the probability for each of the class\n",
    "        a2 = self.softmax(z2)\n",
    "        self.inter['a1'] = a1\n",
    "        self.inter['z1'] = z1\n",
    "        self.inter['a2'] = a2\n",
    "        self.inter['z2'] = z2\n",
    "        self.predict = a2\n",
    "        return self.predict\n",
    "\n",
    "#sigmoid_deriv gives the derivation of the sigmoid function\n",
    "    def sigmoid_deriv(self, x):\n",
    "        return self.sigmoid(x) * (1 - self.sigmoid(x))\n",
    "\n",
    "#Backprop function is to implement the back propogation after calculating the error\n",
    "#Based on the dE/dw for every weight, the weights are updated\n",
    "    def Backprop(self):\n",
    "        m = self.X.shape[0]\n",
    "        #loss calculation \n",
    "        loss = self.lossfunction(self.predict, self.y)\n",
    "        #dz2 = dE/da2*da2/dz2 which is equal to (prediction - Real) for cross entropy and softmax function\n",
    "        dz2 = (self.predict - self.y) / m\n",
    "        da1 = np.dot(dz2, self.param['W2'].T)\n",
    "        dz1 = da1 * self.sigmoid_deriv(self.inter['z1'])\n",
    "\n",
    "        #Updating the weights and bias term based on the dE/dw\n",
    "        self.param['W2'] = self.param['W2'] - self.lr * np.dot(self.inter['a1'].T, dz2)\n",
    "        self.param['b2'] = self.param['b2'] - self.lr * np.sum(dz2, axis=0, keepdims=True)\n",
    "        self.param['W1'] = self.param['W1'] - self.lr * np.dot(self.X.T, dz1)\n",
    "        self.param['b1'] = self.param['b1'] - self.lr * np.sum(dz1, axis=0, keepdims=True)\n",
    "        return loss\n",
    "\n",
    "#prediction function is called after training the model for required number of epochs\n",
    "#It calls the Forward propogation to predict the one hot encoded labels for given inputs\n",
    "#Input is the data points to be predicted and returns the one hot encoded in numpy array\n",
    "    def prediction(self, data):\n",
    "        self.X = data\n",
    "        pred = self.Forward()\n",
    "        pred = np.where(pred > 0.5, 1, 0)\n",
    "        return pred\n",
    "\n",
    "#This function is used to store the weights of the trained model as separate file .npy file\n",
    "    def Weightstore(self):\n",
    "        self.weight = []\n",
    "        for key in self.param.keys():\n",
    "            self.weight.append(self.param[key])\n",
    "        np.save('weights.npy', self.weight)\n",
    "        return\n",
    "\n",
    "#This function enables to implement the mini batch size backpropogation\n",
    "#Rather than feeding whole training data once, input of some batch sizes are fed and then backpopogated to the update weights\n",
    "#This function is being called (totaldata/batchsize) times in 1 Epoch resulting in the weight update for same time in 1 Epoch\n",
    "#Input is batch sized input and fits the model\n",
    "    def mini_batch_implementation(self, data, label):\n",
    "        self.X = data\n",
    "        self.y = label\n",
    "        pred = self.Forward()\n",
    "        loss = self.Backprop()\n",
    "        return pred, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining Batch size and number of epochs for which model is required to train\n",
    "batch_size = 8\n",
    "epochs = 15\n",
    "#Calling the constructor of the NeuralNetwork class\n",
    "nn = NeuralNetwork(X_train[0:batch_size], np.array(y_train[0:batch_size]))\n",
    "#Initializing the weights\n",
    "nn.weightinit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Training Loss 0.0597843176650113\n",
      "Epoch 2 : Training Loss 0.0011780151972225607\n",
      "Epoch 3 : Training Loss 0.0008209587275510438\n",
      "Epoch 4 : Training Loss 0.00035987401293843406\n",
      "Epoch 5 : Training Loss 0.0003283729837512418\n",
      "Epoch 6 : Training Loss 0.00021211293986590646\n",
      "Epoch 7 : Training Loss 0.0007456454246492077\n",
      "Epoch 8 : Training Loss 0.000733538122934998\n",
      "Epoch 9 : Training Loss 0.00041182598860860453\n",
      "Epoch 10 : Training Loss 0.0016278069054302027\n",
      "Epoch 11 : Training Loss 0.0015710918945201153\n",
      "Epoch 12 : Training Loss 0.0008258077595258102\n",
      "Epoch 13 : Training Loss 0.0004603653012098317\n",
      "Epoch 14 : Training Loss 0.0013974648452064968\n",
      "Epoch 15 : Training Loss 0.0012114601083762493\n"
     ]
    }
   ],
   "source": [
    "# These two for loops fits the model in batches of input of size 8 and prints the loss value every Epoch\n",
    "for x in range(epochs):\n",
    "    for a in range(0,len(X_train),batch_size):\n",
    "        pred, loss = nn.mini_batch_implementation(X_train[a:a+batch_size], np.array(y_train[a:a+batch_size]))\n",
    "    print(\"Epoch \" + str(x + 1) + \" : \" + \"Training Loss \" + str(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This class function call stores the weights of the model in the current directory\n",
    "nn.Weightstore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nn.prediction function returns the one hot encoded numpy array\n",
    "prediction_function = nn.prediction(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function calls the prediction function of the class and return the accuracy in percentage\n",
    "def get_accuracy(X , y):\n",
    "    accuracy = 0\n",
    "    y_hat = nn.prediction(X)\n",
    "    for i in range(y_hat.shape[0]):\n",
    "        accuracy += 1 if (y[i] == y_hat[i]).all() else 0\n",
    "    return (accuracy/y.shape[0])*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing the Training and Validation accuracy\n",
    "train_acc = get_accuracy(X_train, np.array(y_train))\n",
    "val_acc = get_accuracy(X_val, np.array(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy :  99.94613520064638\n",
      "Validation accuracy :  99.03069466882067\n"
     ]
    }
   ],
   "source": [
    "print(\"Training accuracy : \", train_acc)\n",
    "print(\"Validation accuracy : \", val_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
